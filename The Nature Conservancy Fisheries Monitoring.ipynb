{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nature Conservancy Fisheries Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I tried to solve the [The Nature Conservancy Fisheries Monitoring](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring) challenge using a simple finetuning approach.\n",
    "\n",
    "Download the challenge's data and put it in the same folder of this notebook, then run all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import zipfile\n",
    "\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = 'data'\n",
    "\n",
    "train_path = 'train.zip'\n",
    "test_stg1_path = 'test_stg1.zip'\n",
    "test_stg2_path = 'test_stg2.7z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_base_dir):\n",
    "    os.makedirs(data_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, out_dir):\n",
    "    #if not os.path.exists(out_dir):\n",
    "    name = os.path.basename(zip_path).split('.')[0]\n",
    "    zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
    "    zip_ref.extractall(out_dir)\n",
    "    zip_ref.close()\n",
    "    return os.path.join(out_dir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract training data\n",
    "orig_train_dir = extract_zip(train_path, data_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract stage_1 test data\n",
    "test_stg1_dir  = extract_zip(test_stg1_path, os.path.join(data_base_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.call(r'\"C:\\Program Files\\7-Zip\\7z.exe\" x ' + test_stg2_path + ' -o' + data_base_dir)\n",
    "except:\n",
    "    print(\"\"\"\n",
    "    Some thing went wrong, maybe you are not using windows or you dont have 7-Zip installed on your machine.\n",
    "    You can ignore this cell and extract the ----> {} <---- file with a dedicated software,\\\n",
    "    then copy the images (not the folder) inside ----> {} <----.\n",
    "    Or, you can uncomment the next cell and run it (not recommended, it takes forever). in this case, \\\n",
    "    extra packages should be installed: py7zlib and tqdm.\n",
    "    Sorry for this inconvenience.\n",
    "    \"\"\".format(test_stg2_path, test_stg1_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tqdm import tqdm_notebook\\nimport py7zlib\\n\\nclass SevenZFile(object):\\n    \\n    def __init__(self, filepath):\\n        fp = open(filepath, 'rb')\\n        self.archive = py7zlib.Archive7z(fp)\\n        \\n    def is_7zfile(cls, filepath):\\n        is7z = False\\n        fp = None\\n        try:\\n            fp = open(filepath, 'rb')\\n            archive = py7zlib.Archive7z(fp)\\n            n = len(archive.getnames())\\n            is7z = True\\n        finally:\\n            if fp:\\n                fp.close()\\n        return is7z\\n\\n    def extractall(self, path):\\n        for name in tqdm_notebook(self.archive.getnames()):\\n            outfilename = os.path.join(path, name)\\n            outdir = os.path.dirname(outfilename)\\n            if not os.path.exists(outdir):\\n                os.makedirs(outdir)\\n            outfile = open(outfilename, 'wb')\\n            outfile.write(self.archive.getmember(name).read())\\n            outfile.close()\\n            \\nSevenZFile(test_stg2_path).extractall(data_base_dir)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract stage_1 test data\n",
    "# this takes forever to run, if you are in a hurry, extract it using the 7zip software\n",
    "\"\"\"\n",
    "from tqdm import tqdm_notebook\n",
    "import py7zlib\n",
    "\n",
    "class SevenZFile(object):\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        fp = open(filepath, 'rb')\n",
    "        self.archive = py7zlib.Archive7z(fp)\n",
    "        \n",
    "    def is_7zfile(cls, filepath):\n",
    "        is7z = False\n",
    "        fp = None\n",
    "        try:\n",
    "            fp = open(filepath, 'rb')\n",
    "            archive = py7zlib.Archive7z(fp)\n",
    "            n = len(archive.getnames())\n",
    "            is7z = True\n",
    "        finally:\n",
    "            if fp:\n",
    "                fp.close()\n",
    "        return is7z\n",
    "\n",
    "    def extractall(self, path):\n",
    "        for name in tqdm_notebook(self.archive.getnames()):\n",
    "            outfilename = os.path.join(path, name)\n",
    "            outdir = os.path.dirname(outfilename)\n",
    "            if not os.path.exists(outdir):\n",
    "                os.makedirs(outdir)\n",
    "            outfile = open(outfilename, 'wb')\n",
    "            outfile.write(self.archive.getmember(name).read())\n",
    "            outfile.close()\n",
    "            \n",
    "SevenZFile(test_stg2_path).extractall(data_base_dir)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stg2_dir = os.path.join(data_base_dir, 'test_stg2')\n",
    "\n",
    "for img in os.listdir(test_stg2_dir):\n",
    "    shutil.move(os.path.join(test_stg2_dir, img), test_stg1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = os.path.join(data_base_dir, 'train_val_split', 'training')\n",
    "validation_dir = os.path.join(data_base_dir, 'train_val_split', 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [class_ for class_ in os.listdir(orig_train_dir) if os.path.isdir(os.path.join(orig_train_dir, class_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in classes:\n",
    "    \n",
    "    class_orig_dir = os.path.join(orig_train_dir, class_)\n",
    "    class_training_dir = os.path.join(training_dir, class_)\n",
    "    class_validation_dir = os.path.join(validation_dir, class_)\n",
    "    \n",
    "    if not os.path.exists(class_training_dir):\n",
    "        os.makedirs(class_training_dir)\n",
    "        \n",
    "    if not os.path.exists(class_validation_dir):\n",
    "        os.makedirs(class_validation_dir)\n",
    "\n",
    "    img_list = os.listdir(class_orig_dir)\n",
    "\n",
    "    for img in img_list:\n",
    "        hash_name = hashlib.sha1(img.encode('ascii'))\n",
    "        if int(hash_name.hexdigest(), 16) % 1000 > 100:\n",
    "            shutil.copy(os.path.join(class_orig_dir, img), class_training_dir)\n",
    "        else:\n",
    "            shutil.copy(os.path.join(class_orig_dir, img), class_validation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning InceptionResnetV2 (trained on imagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are very powerful at learning complex vision tasks, but they often require huge amounts of data to train and may take days to weeks to converge. Unfortunately, data is expensive and difficult to acquire. For most of new applications, no huge amounts of data, nor the required time are available. For example, our 8-classes dataset contains less than 4000 images, clearly insufficient to make a deep neural network learn and discover enough patterns to solve the task. \n",
    "\n",
    "A gentile solution to this issue is transfer learning. The idea is to start with a network that was previously trained on a large dataset, typically on a large-scale image-classification task, and then use it either as an initialization or a fixed feature extractor for the task of interest. \n",
    "* CNNs as a feature extractor: in this case we take the pretrained network, remove the last classification layer, put another one with the proper number of classes and train only this final layer on our dataset\n",
    "* Fine-tuning the neural network:  The second strategy is to not only replace and retrain the classifier on top of the network on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the network, or it is possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network.\n",
    "\n",
    "This is motivated by the observation that the earlier features of a CNN contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the CNN become progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many sea creatures and fish classes, a significant portion of the representational power of the CNN may be devoted to features that are specific to differentiating between fish breeds [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy that I adopted is to take a model pretrained on ImageNet dataset and finetune a part of it on our dataset. The model i have chosen is *__InceptionResNetV2__* This architecture, as for most of CNNs, comprises two parts:  a series of convolutional and pooling layers called the  **convolutional base**, followed by a Global average pooling and a densely connected layer that act as a **classifier**.\n",
    "First, we get rid of the **classification head** that was intended to classify the image into one of the 1000 classes, and we replace it by a global average pooling and a 8 nodes dense layer. We are going to fine-tune some of the last layers, but at the beginning, we need to freeze all the convolutional base and train only the dense layer, the reason is that the weights of the last layer are initialized randomly, thus the error at the beginning is very large due to this total randomness, and backpropagation will send back very large gradient values that would destroy the already trained layers. Once the classifier on top has already been trained, we can unfreeze some of the top convolutional layers and train both these layers and the layer we added. This process is summarized in the list below and illustrated in the figures below (reproduced from [2]).\n",
    "  1. Add custom layers on top of an already-trained base network.\n",
    "  2. Freeze the base network.\n",
    "  3. Train the layers we added.\n",
    "  4. Unfreeze some layers in the base network.\n",
    "  5. Jointly train both these layers and the layer we added.\n",
    "<img src=\"figures/transfer_learning_1.png\" width=\"400\" height=\"200\" />\n",
    "<img src=\"figures/transfer_learning_2.png\" width=\"600\" height=\"400\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_base = InceptionResNetV2(include_top=False) #VGG16(include_top=False) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(conv_base)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint('quicksign_inception_resnet_512.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "Data augmentation is a technique  to get around the lack of training data by creating fake data from the existing ones. It has been a particularly effective technique for image classification. With very simple transformations, we can create thousands of new valid labeled images that makes the trained model robust.\n",
    "The transformations we used are:\n",
    "* rotation\n",
    "* vertical and horizontal shift\n",
    "* zoom\n",
    "* shear\n",
    "* horizontal flip\n",
    "\n",
    "The Keras *ImageDataGenerator* class generates batches of tensor image data with real-time data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3393 images belonging to 8 classes.\n",
      "Found 384 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(training_dir,\n",
    "                                                    target_size=(512, 512),\n",
    "                                                    batch_size=16,\n",
    "                                                    class_mode='categorical')\n",
    "validation_generator = validation_data_gen.flow_from_directory(validation_dir,\n",
    "                                                    target_size=(512, 512),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 277s - loss: 1.6380 - acc: 0.4366 - val_loss: 1.6365 - val_acc: 0.4661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63651, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 2/20\n",
      " - 260s - loss: 1.5654 - acc: 0.4574 - val_loss: 1.6286 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.63651 to 1.62862, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 3/20\n",
      " - 256s - loss: 1.5393 - acc: 0.4554 - val_loss: 1.5856 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.62862 to 1.58559, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 4/20\n",
      " - 261s - loss: 1.5076 - acc: 0.4627 - val_loss: 1.5803 - val_acc: 0.4948\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.58559 to 1.58032, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 5/20\n",
      " - 259s - loss: 1.4808 - acc: 0.4695 - val_loss: 1.5296 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.58032 to 1.52963, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 6/20\n",
      " - 258s - loss: 1.4586 - acc: 0.4801 - val_loss: 1.5300 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.52963\n",
      "Epoch 7/20\n",
      " - 258s - loss: 1.4341 - acc: 0.4918 - val_loss: 1.5096 - val_acc: 0.4974\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.52963 to 1.50962, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 8/20\n",
      " - 260s - loss: 1.4147 - acc: 0.5029 - val_loss: 1.5005 - val_acc: 0.4714\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.50962 to 1.50047, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 9/20\n",
      " - 263s - loss: 1.4017 - acc: 0.5167 - val_loss: 1.4958 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.50047 to 1.49580, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 10/20\n",
      " - 256s - loss: 1.3843 - acc: 0.5203 - val_loss: 1.4973 - val_acc: 0.4453\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.49580\n",
      "Epoch 11/20\n",
      " - 254s - loss: 1.3623 - acc: 0.5314 - val_loss: 1.5231 - val_acc: 0.4219\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.49580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2538dc7c6a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, epochs=20, validation_data=validation_generator, verbose=2,\n",
    "                    callbacks=[checkpointer, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('quicksign_inception_resnet_512.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if 'conv_7b' in layer.name:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 275s - loss: 1.3416 - acc: 0.5364 - val_loss: 1.3056 - val_acc: 0.5599\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.49580 to 1.30556, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 2/100\n",
      " - 267s - loss: 1.2747 - acc: 0.5678 - val_loss: 1.2482 - val_acc: 0.5599\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.30556 to 1.24817, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 3/100\n",
      " - 272s - loss: 1.2328 - acc: 0.5748 - val_loss: 1.2010 - val_acc: 0.5677\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.24817 to 1.20097, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 4/100\n",
      " - 261s - loss: 1.2006 - acc: 0.5872 - val_loss: 1.1665 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.20097 to 1.16649, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 5/100\n",
      " - 254s - loss: 1.1354 - acc: 0.6042 - val_loss: 1.1372 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.16649 to 1.13716, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 6/100\n",
      " - 258s - loss: 1.1210 - acc: 0.6004 - val_loss: 1.1078 - val_acc: 0.5990\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.13716 to 1.10775, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 7/100\n",
      " - 258s - loss: 1.0979 - acc: 0.6127 - val_loss: 1.0806 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.10775 to 1.08057, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 8/100\n",
      " - 253s - loss: 1.0800 - acc: 0.6162 - val_loss: 1.0578 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.08057 to 1.05780, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 9/100\n",
      " - 254s - loss: 1.0536 - acc: 0.6315 - val_loss: 1.0402 - val_acc: 0.6198\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.05780 to 1.04015, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 10/100\n",
      " - 255s - loss: 1.0324 - acc: 0.6373 - val_loss: 1.0210 - val_acc: 0.6328\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.04015 to 1.02099, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 11/100\n",
      " - 254s - loss: 0.9957 - acc: 0.6476 - val_loss: 0.9955 - val_acc: 0.6510\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.02099 to 0.99547, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 12/100\n",
      " - 256s - loss: 0.9822 - acc: 0.6537 - val_loss: 0.9785 - val_acc: 0.6589\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99547 to 0.97850, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 13/100\n",
      " - 253s - loss: 0.9700 - acc: 0.6593 - val_loss: 0.9742 - val_acc: 0.6745\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.97850 to 0.97425, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 14/100\n",
      " - 256s - loss: 0.9516 - acc: 0.6623 - val_loss: 0.9533 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.97425 to 0.95331, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 15/100\n",
      " - 255s - loss: 0.9397 - acc: 0.6725 - val_loss: 0.9398 - val_acc: 0.6745\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.95331 to 0.93983, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 16/100\n",
      " - 254s - loss: 0.9326 - acc: 0.6784 - val_loss: 0.9375 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.93983 to 0.93748, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 17/100\n",
      " - 255s - loss: 0.9126 - acc: 0.6828 - val_loss: 0.9126 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.93748 to 0.91257, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 18/100\n",
      " - 258s - loss: 0.8834 - acc: 0.6992 - val_loss: 0.9074 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.91257 to 0.90737, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 19/100\n",
      " - 254s - loss: 0.8995 - acc: 0.6913 - val_loss: 0.9007 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.90737 to 0.90075, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 20/100\n",
      " - 254s - loss: 0.8736 - acc: 0.7010 - val_loss: 0.8831 - val_acc: 0.7161\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.90075 to 0.88312, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 21/100\n",
      " - 254s - loss: 0.8565 - acc: 0.7075 - val_loss: 0.8657 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.88312 to 0.86571, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 22/100\n",
      " - 255s - loss: 0.8460 - acc: 0.7180 - val_loss: 0.8525 - val_acc: 0.7161\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.86571 to 0.85249, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 23/100\n",
      " - 254s - loss: 0.8456 - acc: 0.7177 - val_loss: 0.8361 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.85249 to 0.83608, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 24/100\n",
      " - 256s - loss: 0.8324 - acc: 0.7127 - val_loss: 0.8406 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.83608\n",
      "Epoch 25/100\n",
      " - 252s - loss: 0.8104 - acc: 0.7230 - val_loss: 0.8217 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.83608 to 0.82172, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 26/100\n",
      " - 253s - loss: 0.7993 - acc: 0.7295 - val_loss: 0.8108 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.82172 to 0.81078, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 27/100\n",
      " - 253s - loss: 0.7807 - acc: 0.7368 - val_loss: 0.8136 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.81078\n",
      "Epoch 28/100\n",
      " - 255s - loss: 0.7885 - acc: 0.7353 - val_loss: 0.8015 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.81078 to 0.80150, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 29/100\n",
      " - 252s - loss: 0.7834 - acc: 0.7350 - val_loss: 0.7952 - val_acc: 0.7448\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.80150 to 0.79520, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 30/100\n",
      " - 253s - loss: 0.7659 - acc: 0.7430 - val_loss: 0.7807 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.79520 to 0.78067, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 31/100\n",
      " - 251s - loss: 0.7561 - acc: 0.7430 - val_loss: 0.7884 - val_acc: 0.7552\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.78067\n",
      "Epoch 32/100\n",
      " - 252s - loss: 0.7605 - acc: 0.7527 - val_loss: 0.7738 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.78067 to 0.77384, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 33/100\n",
      " - 256s - loss: 0.7353 - acc: 0.7620 - val_loss: 0.7624 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.77384 to 0.76241, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 34/100\n",
      " - 253s - loss: 0.7294 - acc: 0.7638 - val_loss: 0.7588 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.76241 to 0.75880, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 35/100\n",
      " - 251s - loss: 0.7318 - acc: 0.7635 - val_loss: 0.7470 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.75880 to 0.74695, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 36/100\n",
      " - 257s - loss: 0.7255 - acc: 0.7650 - val_loss: 0.7365 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.74695 to 0.73651, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 37/100\n",
      " - 253s - loss: 0.7267 - acc: 0.7614 - val_loss: 0.7383 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73651\n",
      "Epoch 38/100\n",
      " - 254s - loss: 0.7030 - acc: 0.7650 - val_loss: 0.7227 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.73651 to 0.72268, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 39/100\n",
      " - 257s - loss: 0.6882 - acc: 0.7697 - val_loss: 0.7148 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.72268 to 0.71482, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 40/100\n",
      " - 252s - loss: 0.7055 - acc: 0.7802 - val_loss: 0.7156 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.71482\n",
      "Epoch 41/100\n",
      " - 253s - loss: 0.7117 - acc: 0.7676 - val_loss: 0.7064 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.71482 to 0.70637, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 42/100\n",
      " - 251s - loss: 0.7072 - acc: 0.7735 - val_loss: 0.7070 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.70637\n",
      "Epoch 43/100\n",
      " - 257s - loss: 0.6868 - acc: 0.7820 - val_loss: 0.6993 - val_acc: 0.7839\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.70637 to 0.69928, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 44/100\n",
      " - 254s - loss: 0.6629 - acc: 0.7899 - val_loss: 0.6921 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.69928 to 0.69211, saving model to quicksign_inception_resnet_512.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      " - 252s - loss: 0.6555 - acc: 0.7878 - val_loss: 0.6936 - val_acc: 0.7839\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.69211\n",
      "Epoch 46/100\n",
      " - 252s - loss: 0.6457 - acc: 0.7984 - val_loss: 0.6879 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.69211 to 0.68785, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 47/100\n",
      " - 253s - loss: 0.6442 - acc: 0.7946 - val_loss: 0.6833 - val_acc: 0.7839\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.68785 to 0.68334, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 48/100\n",
      " - 251s - loss: 0.6354 - acc: 0.8002 - val_loss: 0.6803 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.68334 to 0.68025, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 49/100\n",
      " - 253s - loss: 0.6569 - acc: 0.7905 - val_loss: 0.6772 - val_acc: 0.7943\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.68025 to 0.67716, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 50/100\n",
      " - 254s - loss: 0.6333 - acc: 0.7961 - val_loss: 0.6750 - val_acc: 0.7943\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.67716 to 0.67497, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 51/100\n",
      " - 253s - loss: 0.6217 - acc: 0.8037 - val_loss: 0.6669 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.67497 to 0.66694, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 52/100\n",
      " - 253s - loss: 0.6202 - acc: 0.8022 - val_loss: 0.6653 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.66694 to 0.66532, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 53/100\n",
      " - 257s - loss: 0.6159 - acc: 0.8025 - val_loss: 0.6532 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.66532 to 0.65324, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 54/100\n",
      " - 252s - loss: 0.6128 - acc: 0.8052 - val_loss: 0.6444 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.65324 to 0.64442, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 55/100\n",
      " - 253s - loss: 0.5955 - acc: 0.8116 - val_loss: 0.6430 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.64442 to 0.64302, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 56/100\n",
      " - 253s - loss: 0.6150 - acc: 0.8064 - val_loss: 0.6373 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.64302 to 0.63726, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 57/100\n",
      " - 251s - loss: 0.5982 - acc: 0.8107 - val_loss: 0.6314 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.63726 to 0.63141, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 58/100\n",
      " - 255s - loss: 0.6054 - acc: 0.8058 - val_loss: 0.6314 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.63141 to 0.63138, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 59/100\n",
      " - 252s - loss: 0.5996 - acc: 0.8072 - val_loss: 0.6243 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.63138 to 0.62425, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 60/100\n",
      " - 252s - loss: 0.5900 - acc: 0.8122 - val_loss: 0.6305 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.62425\n",
      "Epoch 61/100\n",
      " - 252s - loss: 0.5858 - acc: 0.8163 - val_loss: 0.6249 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.62425\n",
      "Epoch 62/100\n",
      " - 253s - loss: 0.5879 - acc: 0.8069 - val_loss: 0.6182 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.62425 to 0.61816, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 63/100\n",
      " - 256s - loss: 0.5920 - acc: 0.8116 - val_loss: 0.6118 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.61816 to 0.61183, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 64/100\n",
      " - 252s - loss: 0.5941 - acc: 0.8163 - val_loss: 0.6056 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.61183 to 0.60564, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 65/100\n",
      " - 254s - loss: 0.5768 - acc: 0.8184 - val_loss: 0.6066 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.60564\n",
      "Epoch 66/100\n",
      " - 250s - loss: 0.5764 - acc: 0.8201 - val_loss: 0.6024 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.60564 to 0.60244, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 67/100\n",
      " - 257s - loss: 0.5661 - acc: 0.8222 - val_loss: 0.6005 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.60244 to 0.60046, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 68/100\n",
      " - 256s - loss: 0.5567 - acc: 0.8248 - val_loss: 0.5901 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.60046 to 0.59014, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 69/100\n",
      " - 255s - loss: 0.5385 - acc: 0.8272 - val_loss: 0.5929 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.59014\n",
      "Epoch 70/100\n",
      " - 252s - loss: 0.5416 - acc: 0.8310 - val_loss: 0.5936 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.59014\n",
      "Epoch 71/100\n",
      " - 256s - loss: 0.5472 - acc: 0.8322 - val_loss: 0.5861 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.59014 to 0.58609, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 72/100\n",
      " - 253s - loss: 0.5303 - acc: 0.8383 - val_loss: 0.5917 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.58609\n",
      "Epoch 73/100\n",
      " - 257s - loss: 0.5258 - acc: 0.8330 - val_loss: 0.5778 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.58609 to 0.57777, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 74/100\n",
      " - 256s - loss: 0.5303 - acc: 0.8345 - val_loss: 0.5830 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.57777\n",
      "Epoch 75/100\n",
      " - 254s - loss: 0.5386 - acc: 0.8278 - val_loss: 0.5805 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.57777\n",
      "Epoch 76/100\n",
      " - 254s - loss: 0.5311 - acc: 0.8331 - val_loss: 0.5761 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.57777 to 0.57615, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 77/100\n",
      " - 253s - loss: 0.5363 - acc: 0.8339 - val_loss: 0.5682 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.57615 to 0.56824, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 78/100\n",
      " - 253s - loss: 0.5315 - acc: 0.8363 - val_loss: 0.5662 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.56824 to 0.56625, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 79/100\n",
      " - 255s - loss: 0.5283 - acc: 0.8336 - val_loss: 0.5619 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.56625 to 0.56193, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 80/100\n",
      " - 256s - loss: 0.5180 - acc: 0.8333 - val_loss: 0.5611 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.56193 to 0.56112, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 81/100\n",
      " - 256s - loss: 0.5117 - acc: 0.8422 - val_loss: 0.5611 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.56112\n",
      "Epoch 82/100\n",
      " - 255s - loss: 0.5272 - acc: 0.8392 - val_loss: 0.5538 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.56112 to 0.55377, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 83/100\n",
      " - 252s - loss: 0.4983 - acc: 0.8424 - val_loss: 0.5484 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.55377 to 0.54839, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 84/100\n",
      " - 253s - loss: 0.4975 - acc: 0.8442 - val_loss: 0.5530 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.54839\n",
      "Epoch 85/100\n",
      " - 254s - loss: 0.5065 - acc: 0.8407 - val_loss: 0.5484 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.54839\n",
      "Epoch 86/100\n",
      " - 259s - loss: 0.5019 - acc: 0.8466 - val_loss: 0.5504 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.54839\n",
      "Epoch 87/100\n",
      " - 254s - loss: 0.4879 - acc: 0.8483 - val_loss: 0.5514 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.54839\n",
      "Epoch 88/100\n",
      " - 254s - loss: 0.4857 - acc: 0.8462 - val_loss: 0.5413 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.54839 to 0.54133, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 89/100\n",
      " - 256s - loss: 0.4858 - acc: 0.8451 - val_loss: 0.5406 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.54133 to 0.54059, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 90/100\n",
      " - 255s - loss: 0.4864 - acc: 0.8574 - val_loss: 0.5423 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.54059\n",
      "Epoch 91/100\n",
      " - 255s - loss: 0.4916 - acc: 0.8448 - val_loss: 0.5424 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.54059\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 258s - loss: 0.4691 - acc: 0.8512 - val_loss: 0.5354 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.54059 to 0.53543, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 93/100\n",
      " - 256s - loss: 0.4665 - acc: 0.8562 - val_loss: 0.5299 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.53543 to 0.52991, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 94/100\n",
      " - 253s - loss: 0.4814 - acc: 0.8460 - val_loss: 0.5303 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.52991\n",
      "Epoch 95/100\n",
      " - 257s - loss: 0.4882 - acc: 0.8392 - val_loss: 0.5330 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.52991\n",
      "Epoch 96/100\n",
      " - 260s - loss: 0.4842 - acc: 0.8501 - val_loss: 0.5267 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.52991 to 0.52672, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 97/100\n",
      " - 257s - loss: 0.4781 - acc: 0.8551 - val_loss: 0.5245 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.52672 to 0.52447, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 98/100\n",
      " - 265s - loss: 0.4783 - acc: 0.8542 - val_loss: 0.5138 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.52447 to 0.51381, saving model to quicksign_inception_resnet_512.h5\n",
      "Epoch 99/100\n",
      " - 264s - loss: 0.4670 - acc: 0.8565 - val_loss: 0.5149 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51381\n",
      "Epoch 100/100\n",
      " - 268s - loss: 0.4470 - acc: 0.8685 - val_loss: 0.5143 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2538dbb87b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, epochs=100, validation_data=validation_generator, verbose=2,\n",
    "                    callbacks=[checkpointer, earlystopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_data_gen.flow_from_directory('data/test/',\n",
    "                                                    target_size=(512, 512),\n",
    "                                                    batch_size=64,\n",
    "                                                    class_mode='categorical',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 591s 3s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13153, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "im_names = np.array(os.listdir(os.path.join('data/test', 'test_stg1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_names = ['test_stg2/'+name if 'image' in name else name for name in im_names ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.DataFrame({'image': im_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(data=preds, columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.concat([df_names, df_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_stg2/image_00001.jpg</td>\n",
       "      <td>0.449768</td>\n",
       "      <td>0.045104</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.255778</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.034865</td>\n",
       "      <td>0.198427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_stg2/image_00002.jpg</td>\n",
       "      <td>0.425014</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.504530</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.029566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_stg2/image_00003.jpg</td>\n",
       "      <td>0.836657</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.138526</td>\n",
       "      <td>0.015086</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.005797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_stg2/image_00004.jpg</td>\n",
       "      <td>0.355983</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>0.054720</td>\n",
       "      <td>0.056654</td>\n",
       "      <td>0.063279</td>\n",
       "      <td>0.056551</td>\n",
       "      <td>0.243720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_stg2/image_00005.jpg</td>\n",
       "      <td>0.936777</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.015417</td>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image       ALB       BET       DOL       LAG  \\\n",
       "0  test_stg2/image_00001.jpg  0.449768  0.045104  0.000157  0.000516   \n",
       "1  test_stg2/image_00002.jpg  0.425014  0.004370  0.004676  0.000242   \n",
       "2  test_stg2/image_00003.jpg  0.836657  0.001974  0.001573  0.000087   \n",
       "3  test_stg2/image_00004.jpg  0.355983  0.052375  0.116718  0.054720   \n",
       "4  test_stg2/image_00005.jpg  0.936777  0.010723  0.002031  0.000056   \n",
       "\n",
       "        NoF     OTHER     SHARK       YFT  \n",
       "0  0.255778  0.015385  0.034865  0.198427  \n",
       "1  0.504530  0.013302  0.018300  0.029566  \n",
       "2  0.138526  0.015086  0.000300  0.005797  \n",
       "3  0.056654  0.063279  0.056551  0.243720  \n",
       "4  0.004347  0.011373  0.015417  0.019274  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/transfer-learning/)\n",
    "\n",
    "[2]: Chollet, Francois. Deep learning with python. Manning Publications Co., 2017.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
